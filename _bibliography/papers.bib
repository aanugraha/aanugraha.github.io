%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INBOOK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Inbook{nugraha2018ass,
    bibtex_show = {true},
    author    = {Nugraha, Aditya Arie and Liutkus, Antoine and Vincent, Emmanuel},
    title     = {Deep Neural Network Based Multichannel Audio Source Separation},
    editor    = {Makino, Shoji},
    booktitle = {Audio Source Separation},
    year      = {2018},
    publisher = {Springer},
    address   = {Cham},
    pages     = {157--185},
    doi       = {10.1007/978-3-319-73031-8_7},
    url       = {https://doi.org/10.1007/978-3-319-73031-8_7},
    html      = {https://doi.org/10.1007/978-3-319-73031-8_7},
    preprint  = {https://hal.inria.fr/hal-01633858},
    abstract  = {This chapter presents a multichannel audio source separation framework where deep neural networks (DNNs) are used to model the source spectra and combined with the classical multichannel Gaussian model to exploit the spatial information. The parameters are estimated in an iterative expectation-maximization (EM) fashion and used to derive a multichannel Wiener filter. Different design choices and their impact on the performance are discussed. They include the cost functions for DNN training, the number of parameter updates, the use of multiple DNNs, and the use of weighted parameter updates. Finally, we present its application to a speech enhancement task and a music separation task. The experimental results show the benefit of the multichannel DNN-based approach over a single-channel DNN-based approach and the multichannel nonnegative matrix factorization based iterative EM framework.},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ARTICLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@ARTICLE{bando2021neuralfca,
    selected = {true},
    abbr     = {SPL},
    bibtex_show = {true},
    author   = {Bando, Yoshiaki and Sekiguchi, Kouhei and Masuyama, Yoshiki and Nugraha, Aditya Arie and Fontaine, Mathieu and Yoshii, Kazuyoshi},
    journal  = {{IEEE} Signal Processing Letters}, 
    title    = {Neural Full-Rank Spatial Covariance Analysis for Blind Source Separation}, 
    year     = {2021},
    month    = aug,
    volume   = {28},
    number   = {},
    pages    = {1670--1674},
    url      = {https://ieeexplore.ieee.org/document/9506855},
    html     = {https://ieeexplore.ieee.org/document/9506855},
    pdf      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9506855},
    doi      = {10.1109/LSP.2021.3101699},
    abstract = {This paper describes aneural blind source separation (BSS) method based on amortized variational inference (AVI) of a non-linear generative model of mixture signals. A classical statistical approach to BSS is to fit a linear generative model that consists of spatial and source models representing the inter-channel covariances and power spectral densities of sources, respectively. Although the variational autoencoder (VAE) has successfully been used as a non-linear source model with latent features, it should be pretrained from a sufficient amount of isolated signals. Our method, in contrast, enables the VAE-based source model to be trained only from mixture signals. Specifically, we introduce a neural mixture-to-feature inference model that directly infers the latent features from the observed mixture and integrate it with a neural feature-to-mixture generative model consisting of a full-rank spatial model and a VAE-based source model. All the models are optimized jointly such that the likelihood for the training mixtures is maximized in the framework of AVI. Once the inference model is optimized, it can be used for estimating the latent features of sources included in unseen mixture signals. The experimental results show that the proposed method outperformed the state-of-the-art BSS methods based on linear generative models and was comparable to a method based on supervised learning of the VAE-based sourcemodel.},
}

@article{narendra2010multimedia,
    abbr     = {AEEI},
    bibtex_show = {true},
    title    = {Web based multimedia conference system for digital learning in rural elementary school},
    author   = {Aska Narendra and Aditya Arie Nugraha and Yoanes Bandung and Armein Z. R. Langi and Bambang Pharmasetiawan},
    journal  = {Advances in Electrical Engineering and Informatics},
    year     = {2010},
    month    = {},
    volume   = {III},
    number   = {},
    pages    = {97--104},
    url      = {https://www.researchgate.net/publication/261439213_Web_Based_Multimedia_Conference_System_for_Digital_Learning_in_Rural_Elementary_School},
    html     = {https://www.researchgate.net/publication/261439213_Web_Based_Multimedia_Conference_System_for_Digital_Learning_in_Rural_Elementary_School},
    pdf      = {https://www.researchgate.net/profile/Aditya-Nugraha-11/publication/261439213_Web_Based_Multimedia_Conference_System_for_Digital_Learning_in_Rural_Elementary_School/links/0f3175344db75c8eef000000/Web-Based-Multimedia-Conference-System-for-Digital-Learning-in-Rural-Elementary-School.pdf},
    abstract = {This paper describes the process of designing a web-based multimedia conferencing system that will be used to support digital learning for elementary school in rural areas and implementing them in some network testbeds in Bandung, Subang, and Cianjur. The system must be able to send each of the constituent media, namely video, audio, and other materials (e.g. slide presentations) independently so that the learning process between student and teacher could still be running even if one of the media is absent. In addition, the multimedia conferencing system must also be easily operated independently by an elementary school teacher in rural areas with a minimum computer mastery level. The result is a product that is expected to be useful for improving the quality of primary education especially in rural areas through ICT applications.},
}

@article{nugraha2014dereverb,
    abbr     = {ASMP},
    bibtex_show = {true},
    title    = {Single-channel dereverberation by feature mapping using cascade neural networks for robust distant speaker identification and speech recognition},
    author   = {Aditya Arie Nugraha and Kazumasa Yamamoto and Seiichi Nakagawa},
    journal  = {{EURASIP} Journal on Audio, Speech, and Music Processing},
    year     = {2014},
    month    = apr,
    volume   = {2014},
    number   = {13},
    pages    = {1--31},
    url      = {https://link.springer.com/article/10.1186/1687-4722-2014-13},
    html     = {https://link.springer.com/article/10.1186/1687-4722-2014-13},
    pdf      = {https://link.springer.com/content/pdf/10.1186/1687-4722-2014-13.pdf},
    doi      = {10.1186/1687-4722-2014-13},
    abstract = {We present a feature enhancement method that uses neural networks (NNs) to map the reverberant feature in a log-melspectral domain to its corresponding anechoic feature. The mapping is done by cascade NNs trained using Cascade2 algorithm with an implementation of segment-based normalization. Experiments using speaker identification (SID) and automatic speech recognition (ASR) systems were conducted to evaluate the method. The experiments of SID system was conducted by using our own simulated and real reverberant datasets, while the CENSREC-4 evaluation framework was used as the evaluation for the ASR system. The proposed method could remarkably improve the performance of both systems by using limited stereo data and low speaker-variant data as the training data. From the evaluation using SID, we reached 26.0% and 34.8% of error rate reduction (ERR) relative to the baseline by using simulated and real data, respectively, by using only one pair of utterances for matched condition cases. Then, by using combined dataset containing 15 pairs of utterances by one speaker from three positions in a room, we could reach 93.7% of average identification rate (three known and two unknown positions), which was 42.2% of ERR relative to the use of cepstral mean normalization (CMN). From the evaluation using ASR, by using 40 pairs of utterances as the NN training data, we could reach 78.4% of ERR relative to the baseline by using simulated utterances by five speakers. Moreover, we could reach 75.4% and 71.6% of ERR relative to the baseline by using real utterances by five speakers and one speaker, respectively.},
}

@article{nugraha2016massdnn,
    selected = {true},
    abbr     = {TASLP},
    bibtex_show = {true},
    title    = {Multichannel audio source separation with deep neural networks},
    author   = {Aditya Arie Nugraha and Antoine Liutkus and Emmanuel Vincent},
    journal  = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
    year     = {2016},
    month    = sep,
    volume   = {24},
    number   = {9},
    pages    = {1652--1664},
    url      = {http://ieeexplore.ieee.org/document/7492604},
    html     = {http://ieeexplore.ieee.org/document/7492604},
    preprint = {https://hal.inria.fr/hal-01163369},
    doi      = {10.1109/TASLP.2016.2580946},
    abstract = {This article addresses the problem of multichannel audio source separation. We propose a framework where deep neural networks (DNNs) are used to model the source spectra and combined with the classical multichannel Gaussian model to exploit the spatial information. The parameters are estimated in an iterative expectation-maximization (EM) fashion and used to derive a multichannel Wiener filter. We present an extensive experimental study to show the impact of different design choices on the performance of the proposed technique. We consider different cost functions for the training of DNNs, namely the probabilistically motivated Itakura-Saito divergence, and also Kullback-Leibler, Cauchy, mean squared error, and phase-sensitive cost functions. We also study the number of EM iterations and the use of multiple DNNs, where each DNN aims to improve the spectra estimated by the preceding EM iteration. Finally, we present its application to a speech enhancement problem. The experimental results show the benefit of the proposed multichannel approach over a single-channel DNNbased approach and the conventional multichannel nonnegative matrix factorization based iterative EM algorithm.},
    award    = {6th IEEE Signal Processing Society (SPS) Japan Young Author Best Paper Award},
}

@article{nugraha2020gfvae,
    abbr     = {TASLP},
    bibtex_show = {true},
    title    = {A Flow-Based Deep Latent Variable Model for Speech Spectrogram Modeling and Enhancement},
    author   = {Aditya Arie Nugraha and Kouhei Sekiguchi and Kazuyoshi Yoshii},
    journal  = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
    year     = {2020},
    month    = {},
    volume   = {28},
    number   = {},
    pages    = {1104--1117},
    url      = {https://ieeexplore.ieee.org/document/9028147},
    html     = {https://ieeexplore.ieee.org/document/9028147},
    preprint = {https://www.techrxiv.org/articles/A_Flow-Based_Deep_Latent_Variable_Model_for_Speech_Spectrogram_Modeling_and_Enhancement/12375284},
    doi      = {10.1109/TASLP.2020.2979603},
    abstract = {This paper describes a deep latent variable model of speech power spectrograms and its application to semi-supervised speech enhancement with a deep speech prior. By integrating two major deep generative models, a variational autoencoder (VAE) and a normalizing flow (NF), in a mutually-beneficial manner, we formulate a flexible latent variable model called the NF-VAE that can extract low-dimensional latent representations from high-dimensional observations, akin to the VAE, and does not need to explicitly represent the distribution of the observations, akin to the NF. In this paper, we consider a variant of NF called the generative flow (GF a.k.a. Glow) and formulate a latent variable model called the GF-VAE. We experimentally show that the proposed GF-VAE is better than the standard VAE at capturing fine-structured harmonics of speech spectrograms, especially in the high-frequency range. A similar finding is also obtained when the GF-VAE and the VAE are used to generate speech spectrograms from latent variables randomly sampled from the standard Gaussian distribution. Lastly, when these models are used as speech priors for statistical multichannel speech enhancement, the GF-VAE outperforms the VAE and the GF.},
}

@article{nugraha2020nfiva,
    selected = {true},
    abbr     = {SPL},
    bibtex_show = {true},
    title    = {Flow-Based Independent Vector Analysis for Blind Source Separation},
    author   = {Aditya Arie Nugraha and Kouhei Sekiguchi and Mathieu Fontaine and Yoshiaki Bando and Kazuyoshi Yoshii},
    journal  = {{IEEE} Signal Processing Letters},
    year     = {2020},
    month    = {},
    volume   = {27},
    number   = {},
    pages    = {2173--2177},
    url      = {https://ieeexplore.ieee.org/document/9269436},
    html     = {https://ieeexplore.ieee.org/document/9269436},
    pdf      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9269436},
    doi      = {10.1109/LSP.2020.3039944},
    abstract = {This paper describes a time-varying extension of independent vector analysis (IVA) based on the normalizing flow (NF), called NF-IVA, for determined blind source separation of multichannel audio signals. As in IVA, NF-IVA estimates demixing matrices that transform mixture spectra to source spectra in the complex-valued spatial domain such that the likelihood of those matrices for the mixture spectra is maximized under some non-Gaussian source model. While IVA performs a time-invariant bijective linear transformation, NF-IVA performs a series of time-varying bijective linear transformations (flow blocks) adaptively predicted by neural networks. To regularize such transformations, we introduce a soft volume-preserving (VP) constraint. Given mixture spectra, the parameters of NF-IVA are optimized by gradient descent with backpropagation in an unsupervised manner. Experimental results show that NF-IVA successfully performs speech separation in reverberant environments with different numbers of speakers and microphones and that NF-IVA with the VP constraint outperforms NF-IVA without it, standard IVA with iterative projection, and improved IVA with gradient descent.},
}

@article{sekiguchi2019massvae,
    abbr     = {TASLP},
    bibtex_show = {true},
    title   = {Semi-supervised Multichannel Speech Enhancement with a Deep Speech Prior},
    author  = {Kouhei Sekiguchi and Yoshiaki Bando and Aditya Arie Nugraha and Kazuyoshi Yoshii and Tatsuya Kawahara},
    journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
    year    = {2019},
    month   = dec,
    volume  = {27},
    number  = {12},
    pages   = {2197--2212},
    url     = {http://ieeexplore.ieee.org/document/8861142},
    html    = {http://ieeexplore.ieee.org/document/8861142},
    pdf     = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8861142},
    code    = {https://github.com/sekiguchi92/TASLP2019},
    doi     = {10.1109/TASLP.2019.2944348},
    abstract = {This paper describes a semi-supervised multichannel speech enhancement method that only uses clean speech data for prior training. Although multichannel nonnegative matrix factorization (MNMF) and its constrained variant called independent low-rank matrix analysis (ILRMA) have successfully been used for unsupervised speech enhancement, the low-rank assumption on the power spectral densities (PSDs) of all sources (speech and noise) does not hold in reality. To solve this problem, we replace a low-rank model of speech with a deep generative model in the framework of MNMF or ILRMA, i.e., formulate a probabilistic model of noisy speech by integrating a deep speech model, a low-rank noise model, and a full-rank or rank-1 model of spatial characteristics of speech and noise. The deep speech model is trained from clean speech data in an unsupervised auto-encoding variational Bayesian manner. Given multichannel noisy speech spectra, the full-rank or rank-1 spatial covariance matrices and PSDs of speech and noise are estimated in an unsupervised maximum-likelihood manner. Experimental results showed that the full-rank version of the proposed method was significantly better than MNMF, ILRMA, and the rank-1 version. We confirmed that the initialization-sensitivity and local-optimum problems of MNMF with many spatial parameters can be solved by incorporating the precise speech model.},
    award    = {17th IEEE Kansai Section Student Paper Award},
}

@article{sekiguchi2020fastmnmf,
    selected = {true},
    abbr     = {TASLP},
    bibtex_show = {true},
    title    = {Fast Multichannel Nonnegative Matrix Factorization with Directivity-Aware Jointly-Diagonalizable Spatial Covariance Matrices for Blind Source Separation},
    author   = {Kouhei Sekiguchi and Yoshiaki Bando and Aditya Arie Nugraha and Kazuyoshi Yoshii and Tatsuya Kawahara},
    journal  = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
    year     = {2020},
    month    = aug,
    volume   = {28},
    number   = {},
    pages    = {2610--2625},
    url      = {https://ieeexplore.ieee.org/document/9177266},
    html     = {https://ieeexplore.ieee.org/document/9177266},
    pdf      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9177266},
    code     = {https://github.com/sekiguchi92/SoundSourceSeparation},
    doi      = {10.1109/TASLP.2020.3019181},
    abstract = {This paper describes a computationally-efficient blind source separation (BSS) method based on the independence, low-rankness, and directivity of the sources. A typical approach to BSS is unsupervised learning of a probabilistic model that consists of a source model representing the time-frequency structure of source images and a spatial model representing their inter-channel covariance structure. Building upon the low-rank source model based on nonnegative matrix factorization (NMF), which has been considered to be effective for inter-frequency source alignment, multichannel NMF (MNMF) assumes source images to follow multivariate complex Gaussian distributions with unconstrained full-rank spatial covariance matrices (SCMs). An effective way of reducing the computational cost and initialization sensitivity of MNMF is to restrict the degree of freedom of SCMs. While a variant of MNMF called independent low-rank matrix analysis (ILRMA) severely restricts SCMs to rank-1 matrices under an idealized condition that only directional and less-echoic sources exist, we restrict SCMs to jointly-diagonalizable yet full-rank matrices in a frequency-wise manner, resulting in FastMNMF1. To help inter-frequency source alignment, we then propose FastMNMF2 that shares the directional feature of each source over all frequency bins. To explicitly consider the directivity or diffuseness of each source, we also propose rank-constrained FastMNMF that enables us to individually specify the ranks of SCMs. Our experiments showed the superiority of FastMNMF over MNMF and ILRMA in speech separation and the effectiveness of the rank constraint in speech enhancement.},
    award    = {15th IEEE Signal Processing Society (SPS) Japan Student Journal Paper Award},
}

@article{vincent2017csl,
    selected = {true},
    abbr     = {CSL},
    bibtex_show = {true},
    title    = {An analysis of environment, microphone and data simulation mismatches in robust speech recognition},
    author   = {Emmanuel Vincent and Shinji Watanabe and Aditya Arie Nugraha and Jon Barker and Ricard Marxer},
    journal  = {Computer Speech & Language},
    year     = {2017},
    month    = nov,
    volume   = {46},
    number   = {9},
    pages    = {535--557},
    url      = {http://www.sciencedirect.com/science/article/pii/S0885230816301231},
    html     = {http://www.sciencedirect.com/science/article/pii/S0885230816301231},
    preprint = {https://hal.inria.fr/hal-01399180},
    doi      = {10.1016/j.csl.2016.11.005},
    abstract = {Speech enhancement and automatic speech recognition (ASR) are most often evaluated in matched (or multi-condition) settings where the acoustic conditions of the training data match (or cover) those of the test data. Few studies have systematically assessed the impact of acoustic mismatches between training and test data, especially concerning recent speech enhancement and state-of-the-art ASR techniques. In this article, we study this issue in the context of the CHiME-3 dataset, which consists of sentences spoken by talkers situated in challenging noisy environments recorded using a 6-channel tablet based microphone array. We provide a critical analysis of the results published on this dataset for various signal enhancement, feature extraction, and ASR backend techniques and perform a number of new experiments in order to separately assess the impact of different noise environments, different numbers and positions of microphones, or simulated vs. real data on speech enhancement and ASR performance. We show that, with the exception of minimum variance distortionless response (MVDR) beamforming, most algorithms perform consistently on real and simulated data and can benefit from training on simulated data. We also find that training on different noise environments and different microphones barely affects the ASR performance, especially when several environments are present in the training data: only the number of microphones has a significant impact. Based on these results, we introduce the CHiME-4 Speech Separation and Recognition Challenge, which revisits the CHiME-3 dataset and makes it more challenging by reducing the number of microphones available for testing.},
    award = {ISCA Award for the Best Review Paper published in Computer Speech and Language (2016-2020)},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INPROCEEDINGS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@InProceedings{chau19roman,
    abbr     = {RO-MAN},
    bibtex_show = {true},
    author    = {Aaron Chau and Kouhei Sekiguchi and Aditya Arie Nugraha and Kazuyoshi Yoshii and Kotaro Funakoshi},
    title     = {Audio-Visual SLAM towards Human Tracking and Human-Robot Interaction in Indoor Environments},
    booktitle = {Proceedings of IEEE International Conference on Robot \& Human Interactive Communication (RO-MAN)},
    month     = oct,
    year      = {2019},
    pages     = {1--8},
    address   = {New Delhi, India},
    url       = {https://ieeexplore.ieee.org/document/8956321},
    html      = {https://ieeexplore.ieee.org/document/8956321},
    preprint  = {http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/papers/roman-2019-chau.pdf},
    doi       = {10.1109/RO-MAN46459.2019.8956321},
    abstract  = {We propose a novel audio-visual simultaneous and localization (SLAM) framework that exploits human pose and acoustic speech of human partners to allow a robot equipped with a microphone array and a monocular camera to track, map, and interact with human sound sources in an indoor environment. Since human interaction is characterized by features perceived in not only the visual modality, but the acoustic modality as well. SLAM systems must utilize information from both modalities. Using a state-of-the-art beamforming technique, we obtain sound components corresponding to speech and noise, and estimate the Direction-of-Arrival (DoA) estimates of active sound sources as useful representations of observed features in the acoustic modality. Through estimated human pose by a monocular camera, we obtain the relative positions of humans as useful representation of observed features in the visual modality. Using these techniques, we attempt to eliminate restrictions imposed by intermittent speech, noisy and reverberant periods, triangulation of sound-source range, and restrictions imposed by limited visual field-of-views; and subsequently perform early fusion on these representations. We develop a system that allows for complimentary action between audio-visual sensor modalities in the simultaneous mapping of multiple human sound sources and the localization of observer position.},
    award     = {Best Conference Paper Award},
}

@INPROCEEDINGS{du2020phonespeakeraware,
    abbr     = {EUSIPCO},
    bibtex_show = {true},
    author    = {Du, Yicheng and Sekiguchi, Kouhei and Bando, Yoshiaki and Arie Nugraha, Aditya and Fontaine, Mathieu and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
    title     = {Semi-supervised Multichannel Speech Separation Based on a Phone- and Speaker-Aware Deep Generative Model of Speech Spectrograms}, 
    booktitle = {Proceedings of European Signal Processing Conference (EUSIPCO)},
    year      = {2020},
    OPTmonth  = jan,
    pages     = {870--874},
    address   = {Amsterdam, Netherlands},
    url       = {https://ieeexplore.ieee.org/document/9287464},
    html      = {https://ieeexplore.ieee.org/document/9287464},
    pdf       = {https://www.eurasip.org/Proceedings/Eusipco/Eusipco2020/pdfs/0000870.pdf},
    doi       = {10.23919/Eusipco47968.2020.9287464},
    abstract  = {This paper describes a semi-supervised multichannel speech separation method that uses clean speech signals with frame-wise phonetic labels and sample-level speaker labels for pre-training. A standard approach to statistical source separation is to formulate a probabilistic model of multichannel mixture spectrograms that combines source models representing the time-frequency characteristics of sources with spatial models representing the covariance structure between channels. For speech separation and enhancement, deep generative models with latent variables have successfully been used as source models. The parameters of such a speech model can be trained beforehand from clean speech signals with a variational autoencoder (VAE) or its conditional variant (CVAE) that takes speaker labels as auxiliary inputs. Because human speech is characterized by both phonetic features and speaker identities, we propose a probabilistic model that combines a phone- and speaker-aware deep speech model with a full-rank spatial model. Our speech model is trained with a CVAE taking both phone and speaker labels as conditions. Given speech mixtures, the spatial covariance matrices, latent variables of sources, and phone and speaker labels of sources are jointly estimated. Comparative experimental results showed that the performance of speech separation can be improved by explicitly considering phonetic features and/or speaker identities.},
}

@InProceedings{fontaine2019eusipco,
    abbr     = {EUSIPCO},
    bibtex_show = {true},
    author    = {Mathieu Fontaine and Aditya Arie Nugraha and Roland Badeau and Kazuyoshi Yoshii and Antoine Liutkus},
    title     = {Cauchy Multichannel Speech Enhancement with a Deep Speech Prior},
    booktitle = {Proceedings of European Signal Processing Conference (EUSIPCO)},
    month     = sep,
    year      = {2019},
    pages     = {1--5},
    address   = {A Coru\~{n}a, Spain},
    url       = {https://ieeexplore.ieee.org/document/8903091},
    html      = {https://ieeexplore.ieee.org/document/8903091},
    preprint  = {https://hal.telecom-paristech.fr/hal-02288063},
    slides    = {https://matfontaine.github.io/EUSIPCO-presentation},
    doi       = {10.23919/EUSIPCO.2019.8903091},
    abstract  = {We propose a semi-supervised multichannel speech enhancement system based on a probabilistic model which assumes that both speech and noise follow the heavy-tailed multivariate complex Cauchy distribution. As we advocate, this allows handling strong and adverse noisy conditions. Consequently, the model is parameterized by the source magnitude spectrograms and the source spatial scatter matrices. To deal with the non-additivity of scatter matrices, our first contribution is to perform the enhancement on a projected space. Then, our second contribution is to combine a latent variable model for speech, which is trained by following the variational autoencoder framework, with a low-rank model for the noise source. At test time, an iterative inference algorithm is applied, which produces estimated parameters to use for separation. The speech latent variables are estimated first from the noisy speech and then updated by a gradient descent method, while a majorization-equalization strategy is used to update both the noise and the spatial parameters of both sources. Our experimental results show that the Cauchy model outperforms the state-of-art methods. The standard deviation scores also reveal that the proposed method is more robust against non-stationary noise.},
}

@inproceedings{fontaine2020alphastablefastmnmf,
    abbr      = {Interspeech},
    bibtex_show = {true},
    author    = {Mathieu Fontaine and Kouhei Sekiguchi and Aditya Arie Nugraha and Kazuyoshi Yoshii},
    title     = {Unsupervised Robust Speech Enhancement Based on Alpha-Stable Fast Multichannel Nonnegative Matrix Factorization},
    booktitle = {Proceedings of Interspeech},
    year      = {2020},
    month     = oct,
    pages     = {4541--4545},
    address   = {Shanghai, China},
    url       = {https://www.isca-speech.org/archive/interspeech_2020/fontaine20_interspeech.html},
    html      = {https://www.isca-speech.org/archive/interspeech_2020/fontaine20_interspeech.html},
    pdf       = {https://www.isca-speech.org/archive/pdfs/interspeech_2020/fontaine20_interspeech.pdf},
    doi       = {10.21437/Interspeech.2020-3202},
    abstract  = {This paper describes multichannel speech enhancement based on a probabilistic model of complex source spectrograms for improving the intelligibility of speech corrupted by undesired noise. The univariate complex Gaussian model with the reproductive property supports the additivity of source complex spectrograms and forms the theoretical basis of nonnegative matrix factorization (NMF). Multichannel NMF (MNMF) is an extension of NMF based on the multivariate complex Gaussian model with spatial covariance matrices (SCMs), and its state-of-the-art variant called FastMNMF with jointly-diagonalizable SCMs achieves faster decomposition based on the univariate Gaussian model in the transformed domain where all time-frequency-channel elements are independent. Although a heavy-tailed extension of FastMNMF has been proposed to improve the robustness against impulsive noise, the source additivity has never been considered. The multivariate α-stable distribution does not have the reproductive property for the shape matrix parameter. This paper, therefore, proposes a heavy-tailed extension called α-stable FastMNMF which works in the transformed domain to use a univariate complex α-stable model, satisfying the reproductive property for any tail lightness parameter α and allowing the α-fractional Wiener filtering based on the element-wise source additivity. The experimental results show that α-stable FastMNMF with α = 1.8 significantly outperforms Gaussian FastMNMF (α=2).},
}

@inproceedings{fontaine2021alphaarfastmnmf,
    abbr      = {Interspeech},
    bibtex_show = {true},
    author    = {Mathieu Fontaine and Kouhei Sekiguchi and Aditya Arie Nugraha and Yoshiaki Bando and Kazuyoshi Yoshii},
    title     = {Alpha-Stable Autoregressive Fast Multichannel Nonnegative Matrix Factorization for Joint Speech Enhancement and Dereverberation},
    booktitle = {Proceedings of Interspeech},
    year      = {2021},
    month     = aug,
    pages     = {661--665},
    address   = {Brno, Czechia},
    url       = {https://www.isca-speech.org/archive/interspeech_2021/fontaine21_interspeech.html},
    html      = {https://www.isca-speech.org/archive/interspeech_2021/fontaine21_interspeech.html},
    pdf       = {https://www.isca-speech.org/archive/pdfs/interspeech_2021/fontaine21_interspeech.pdf},
    doi       = {10.21437/Interspeech.2021-742},
    abstract  = {This paper proposes α-stable autoregressive fast multichannel nonnegative matrix factorization (α-AR-FastMNMF), a robust joint blind speech enhancement and dereverberation method for improved automatic speech recognition in a realistic adverse environment. The state-of-the-art versatile blind source separation method called FastMNMF that assumes the short-time Fourier transform (STFT) coefficients of a direct sound to follow a circular complex Gaussian distribution with jointly-diagonalizable full-rank spatial covariance matrices was extended to AR-FastMNMF with an autoregressive reverberation model. Instead of the light-tailed Gaussian distribution, we use the heavy-tailed α-stable distribution, which also has the reproductive property useful for the additive source modeling, to better deal with the large dynamic range of the direct sound. The experimental results demonstrate that the proposed α-AR-FastMNMF works well as a front-end of an automatic speech recognition system. It outperforms α-AR-ILRMA, which is a special case of α-AR-FastMNMF, and their Gaussian counterparts, i.e., AR-FastMNMF and AR-ILRMA, in terms of the speech signal quality metrics and word error rate.},
}

@INPROCEEDINGS{gondokaryono2011avstreaming,
    abbr      = {TSSA},
    bibtex_show = {true},
    author    = {Gondokaryono, Yudi Satria and Bandung, Yoanes and Wibowo, Joko Ari and Nugraha, Aditya Arie and Yonathan, Bryan and Ramadhianto, Dwi},
    title     = {Performance evaluation of audio-video streaming service in Keerom, Papua using integrated audio-video performance test tool}, 
    booktitle = {Proceedings of International Conference on Telecommunication Systems, Services, and Applications (TSSA)},
    year      = {2011},
    month     = oct,
    pages     = {145--148},
    address   = {Denpasar, Indonesia},
    url       = {https://ieeexplore.ieee.org/document/6095423},
    html      = {https://ieeexplore.ieee.org/document/6095423},
    doi       = {10.1109/TSSA.2011.6095423},
    abstract  = {This study compared some video codec, audio codec, audio bit rate, video bit rate to determine the quality of the audio-video streaming service on the network Keerom, Papua. Average capacity in this network is 1.5Mbps. Mpeg audio and ac3 are choosen because of its characteristic, while the video codec is mpeg4 and H.264. Audio bit rate used 64 and 128kbps, while the video bit rate 64, 128 and 256kbps. The experiments result show the quality of the audio-video streaming service was better when the audio codec used mpeg audio 64kbps-mpeg4 256kbps. The test results will be used as a reference implementation of audio-video streaming service later in the network Keerom, Papua.},
}

@InProceedings{nugraha2012speaker,
    abbr      = {ASJ},
    bibtex_show = {true},
    author    = {Aditya Arie Nugraha and Seiichi Nakagawa},
    title     = {Improving distant speaker identification robustness using a nonlinear regression based dereverberation method in feature domain},
    booktitle = {Proceedings of the Autumn Meeting of Acoustical Society of Japan},
    year      = {2012},
    month     = sep,
    pages     = {163--166},
    abstract  = {The use of reverberated speech signal which is captured by distant-talking microphone as input of speaker identification system would degrade its performance. In this paper, we present a single-channel non-linear regression based dereverberation method that works on a feature domain. Artificial neural networks were trained using Cascade 2 algorithm on stereo data to compensate the reverberation effect by mapping the reverberated signal to the clean signal on 24-dimensional log-melspectral features. We also employ segment-level normalization to compensate the power difference between the clean signal and the reverberated signal. Using the proposed method, we could enhance the signal and improve the identification rate of distant speaker identification system.}
}

@InProceedings{nugraha2013dereverb,
    abbr     = {APSIPA},
    bibtex_show = {true},
    author   = {Aditya Arie Nugraha and Kazumasa Yamamoto and Seiichi Nakagawa},
    title     = {Single channel dereverberation method in logmelspectral domain using limited stereo data for distant speaker identification},
    booktitle = {Proceedings of Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
    year      = {2013},
    month     = oct,
    pages     = {1--4},
    address   = {Kaohsiung, Taiwan},
    pdf       = {http://www.apsipa.org/proceedings_2013/papers/119_Single-Channel-Nugraha-2927269.pdf},
    abstract  = {In this paper, we present a feature enhancement method that uses neural networks (NNs) to map the reverberant feature in a log-melspectral domain to its corresponding anechoic feature. The mapping is done by cascade NNs trained using Cascade2 algorithm with an implementation of segment-based normalization. We assumed that the dimensions of feature were independent from each other and experimented on several assumptions of the room transfer function for each dimension. Speaker identification system was used to evaluate the method. Using limited stereo data, we could improve the identification rate for simulated and real datasets. On the simulated dataset, we could show that the proposed method is effective for both noiseless and noisy reverberant environments, with various noise and reverberation characteristics. On the real dataset, we could show that by using 6 independent NNs configuration for 24-dimensional feature and only 1 pair of utterances we could get 35% average error reduction relative to the baseline, which employed cepstral mean normalization (CMN).},
}

@InProceedings{nugraha2016eusipco,
    abbr     = {EUSIPCO},
    bibtex_show = {true},
    author    = {Aditya Arie Nugraha and Antoine Liutkus and Emmanuel Vincent},
    title     = {Multichannel music separation with deep neural networks},
    booktitle = {Proceedings of European Signal Processing Conference (EUSIPCO)},
    month     = aug,
    year      = {2016},
    pages     = {1748--1752},
    address   = {Budapest, Hungary},
    url       = {https://ieeexplore.ieee.org/document/7760548},
    html      = {https://ieeexplore.ieee.org/document/7760548},
    preprint  = {https://hal.inria.fr/hal-01334614},
    doi       = {10.1109/EUSIPCO.2016.7760548},
    abstract  = {This article addresses the problem of multichannel music separation. We propose a framework where the source spectra are estimated using deep neural networks and combined with spatial covariance matrices to encode the source spatial characteristics. The parameters are estimated in an iterative expectation-maximization fashion and used to derive a multichannel Wiener filter. We evaluate the proposed framework for the task of music separation on a large dataset. Experimental results show that the method we describe performs consistently well in separating singing voice and other instruments from realistic musical mixtures.},
}

@InProceedings{nugraha19icassp,
    abbr     = {ICASSP},
    bibtex_show = {true},
    author    = {Aditya Arie Nugraha and Kouhei Sekiguchi and Kazuyoshi Yoshii},
    title     = {A Deep Generative Model of Speech Complex Spectrograms},
    booktitle = {Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    month     = may,
    year      = {2019},
    pages     = {905--909},
    address   = {Brighton, UK},
    url       = {https://ieeexplore.ieee.org/document/8682797},
    html      = {https://ieeexplore.ieee.org/document/8682797},
    preprint  = {https://arxiv.org/abs/1903.03269},
    poster    = {https://sigport.org/documents/deep-generative-model-speech-complex-spectrograms},
    doi       = {10.1109/ICASSP.2019.8682797},
    abstract  = {This paper proposes an approach to the joint modeling of the short-time Fourier transform magnitude and phase spectrograms with a deep generative model. We assume that the magnitude follows a Gaussian distribution and the phase follows a von Mises distribution. To improve the consistency of the phase values in the time-frequency domain, we also apply the von Mises distribution to the phase derivatives, i.e., the group delay and the instantaneous frequency. Based on these assumptions, we explore and compare several combinations of loss functions for training our models. Built upon the variational autoencoder framework, our model consists of three convolutional neural networks acting as an encoder, a magnitude decoder, and a phase decoder. In addition to the latent variables, we propose to also condition the phase estimation on the estimated magnitude. Evaluated for a time-domain speech reconstruction task, our models could generate speech with a high perceptual quality and a high intelligibility.},
}

@InProceedings{sekiguchi2019eusipco,
    abbr     = {EUSIPCO},
    bibtex_show = {true},
    author    = {Kouhei Sekiguchi and Aditya Arie Nugraha and Yoshiaki Bando and Kazuyoshi Yoshii},
    title     = {Fast Multichannel Source Separation Based on Jointly Diagonalizable Spatial Covariance Matrices},
    booktitle = {Proceedings of European Signal Processing Conference (EUSIPCO)},
    month     = sep,
    year      = {2019},
    pages     = {1--5},
    address   = {A Coru\~{n}a, Spain},
    url       = {https://ieeexplore.ieee.org/document/8902557},
    html      = {https://ieeexplore.ieee.org/document/8902557},
    preprint  = {https://arxiv.org/abs/1903.03237},
    code      = {https://github.com/sekiguchi92/eusipco2019},
    doi       = {10.23919/EUSIPCO.2019.8902557},
    abstract  = {This paper describes a versatile method that accelerates multichannel source separation methods based on full-rank spatial modeling. A popular approach to multichannel source separation is to integrate a spatial model with a source model for estimating the spatial covariance matrices (SCMs) and power spectral densities (PSDs) of each sound source in the time-frequency domain. One of the most successful examples of this approach is multichannel nonnegative matrix factorization (MNMF) based on a full-rank spatial model and a low-rank source model. MNMF, however, is computationally expensive and often works poorly due to the difficulty of estimating the unconstrained full-rank SCMs. Instead of restricting the SCMs to rank-1 matrices with the severe loss of the spatial modeling ability as in independent low-rank matrix analysis (ILRMA), we restrict the SCMs of each frequency bin to jointly-diagonalizable but still full-rank matrices. For such a fast version of MNMF, we propose a computationally-efficient and convergence-guaranteed algorithm that is similar in form to that of ILRMA. Similarly, we propose a fast version of a state-of-the-art speech enhancement method based on a deep speech model and a low-rank noise model. Experimental results showed that the fast versions of MNMF and the deep speech enhancement method were several times faster and performed even better than the original versions of those methods, respectively.},
}

@INPROCEEDINGS{sekiguchi2021arfastmnmf,
    abbr     = {ICASSP},
    bibtex_show = {true},
    author    = {Sekiguchi, Kouhei and Bando, Yoshiaki and Nugraha, Aditya Arie and Fontaine, Mathieu and Yoshii, Kazuyoshi},
    title     = {Autoregressive Fast Multichannel Nonnegative Matrix Factorization For Joint Blind Source Separation And Dereverberation}, 
    booktitle = {Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    year      = {2021},
    month     = jun,
    pages     = {511--515},
    address   = {Toronto, Canada},
    url       = {https://ieeexplore.ieee.org/document/9414857},
    html      = {https://ieeexplore.ieee.org/document/9414857},
    doi       = {10.1109/ICASSP39728.2021.9414857},
    abstract  = {This paper describes a joint blind source separation and dereverberation method that works adaptively and efficiently in a reverberant noisy environment. The modern approach to blind source separation (BSS) is to formulate a probabilistic model of multichannel mixture signals that consists of a source model representing the time-frequency structures of source spectrograms and a spatial model representing the inter-channel covariance structures of source images. The cutting-edge BSS method in this thread of research is fast multi-channel nonnegative matrix factorization (FastMNMF) that consists of a low-rank source model based on nonnegative matrix factorization (NMF) and a full-rank spatial model based on jointly-diagonalizable spatial covariance matrices. Although FastMNMF is computationally efficient and can deal with both directional sources and diffuse noise simultaneously, its performance is severely degraded in a reverberant environment. To solve this problem, we propose autoregressive FastMNMF (AR-FastMNMF) based on a unified probabilistic model that combines FastMNMF with a blind dereverberation method called weighted prediction error (WPE), where all the parameters are optimized jointly such that the likelihood for observed reverberant mixture signals is maximized. Experimental results showed the superiority of AR-FastMNMF over conventional methods that perform blind dereverberation and BSS jointly or sequentially.},
}

@InProceedings{sivasankaran2015asru,
    abbr     = {ASRU},
    bibtex_show = {true},
    author    = {Sunit Sivasankaran and Aditya Arie Nugraha and Emmanuel Vincent and Juan Andrés Morales Cordovilla and Siddharth Dalmia and Irina Illina and Antoine Liutkus},
    title     = {Robust {ASR} using neural network based speech enhancement and feature simulation},
    booktitle = {Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
    month     = dec,
    year      = {2015},
    pages     = {482--489},
    address   = {Scottsdale, USA},
    url       = {https://ieeexplore.ieee.org/document/7404834},
    html      = {https://ieeexplore.ieee.org/document/7404834},
    preprint  = {https://hal.inria.fr/hal-01204553},
    doi       = {10.1109/ASRU.2015.7404834},
    abstract  = {We consider the problem of robust automatic speech recognition (ASR) in the context of the CHiME-3 Challenge. The proposed system combines three contributions. First, we propose a deep neural network (DNN) based multichannel speech enhancement technique, where the speech and noise spectra are estimated using a DNN based regressor and the spatial parameters are derived in an expectation-maximization (EM) like fashion. Second, a conditional restricted Boltzmann machine (CRBM) model is trained using the obtained enhanced speech and used to generate simulated training and development datasets. The goal is to increase the similarity between simulated and real data, so as to increase the benefit of multicondition training. Finally, we make some changes to the ASR backend. Our system ranked 4th among 25 entries.},
}

@INPROCEEDINGS{yoshii2020fastmctf,
    abbr     = {EUSIPCO},
    bibtex_show = {true},
    author    = {Yoshii, Kazuyoshi and Sekiguchi, Kouhei and Bando, Yoshiaki and Fontaine, Mathieu and Nugraha, Aditya Arie},
    title     = {Fast Multichannel Correlated Tensor Factorization for Blind Source Separation}, 
    booktitle = {Proceedings of European Signal Processing Conference (EUSIPCO)},
    year      = {2020},
    OPTmonth  = jan,
    pages     = {306--310},
    address   = {Amsterdam, Netherlands},
    url       = {https://ieeexplore.ieee.org/document/9287530},
    html      = {https://ieeexplore.ieee.org/document/9287530},
    pdf       = {https://www.eurasip.org/Proceedings/Eusipco/Eusipco2020/pdfs/0000306.pdf},
    doi       = {10.23919/Eusipco47968.2020.9287530},
    abstract  = {This paper describes an ultimate covariance-aware multichannel extension of nonnegative matrix factorization (NMF) for blind source separation (BSS). A typical approach to BSS is to integrate a low-rank source model with a full-rank spatial model as multichannel NMF (MNMF) based on full-rank spatial covariance matrices (CMs) or its efficient version named FastMNMF based on jointly-diagonalizable spatial CMs do. The NMF-based phase-unaware source model, however, can deal with only the positive cooccurrence relations between time-frequency bins. To overcome this limitation, we propose an efficient multichannel extension of correlated tensor factorization (CTF) named FastMCTF based on jointly-diagonalizable temporal, frequency, and spatial CMs. Integration of the jointly-diagonalizable full-rank source model proposed by FastCTF with the jointly-diagonalizable full-rank spatial model proposed by FastMNMF enables us to completely consider the positive and negative covariance relations between frequency bins, time frames, and channels. We derive a convergence-guaranteed parameter estimation algorithm based on the multiplicative update and iterative projection and experimentally show the potential of the proposed method.},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TECHREPORT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@techreport{nugraha2013dereverb,
    abbr        = {SP/IPSJ-SLP},
    bibtex_show = {true},
    author      = {Aditya Arie Nugraha and Kazumasa Yamamoto and Seiichi Nakagawa},
    title       = {Single Channel Dereverberation Method by Feature Mapping Using Limited Stereo Data},
    institution = {Institute of Electronics, Information and Communication Engineers (IEICE)},
    year        = {2013},
    month       = jul, 
    key         = {SP2013-54},
    volume      = {113},
    number      = {161},
    pages       = {7--12},
    url         = {https://www.ieice.org/ken/paper/20130725FB4S/eng/},
    html        = {https://www.ieice.org/ken/paper/20130725FB4S/eng/},
    abstract    = {In this paper, we present a feature enhancement method that uses neural networks (NNs) to map the reverberant feature in a log-melspectral domain to its corresponding anechoic feature. The mapping is done by cascade NNs trained using Cascade2 algorithm with an implementation of segment-based normalization. Experiments using speaker identification (SID) and speech recognition (ASR) systems were conducted to evaluate the method. The experiments of SID system was conducted by using real noisy reverberant datasets, while CENSREC-4 evaluation framework was used as the evaluation for the ASR system. Using limited stereo data consisting of simultaneously recorded clean speech and reverberant speech, the proposed method could remarkably improve the performance of both systems.},
}

