<!DOCTYPE html>
<html lang="en">

  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Aditya Arie Nugraha


</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous">

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css">

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://aanugraha.github.io/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/cv/">
                curriculum vitae
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/demo/">
                demo
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    
    <table cellpadding="0px" cellspacing="0px" border="0px">
      <tr>
        <td style="text-align: center; vertical-align: middle;">
</td>
        <td style="text-align: center; vertical-align: middle;">
<h5>/ˈaː.ri/</h5>
</td>
        <td style="text-align: center; vertical-align: middle;">
</td>
      </tr>
      <tr>
        <td style="padding-right: 6px; text-align: center; vertical-align: middle;">
<h1 class="post-title"> Aditya </h1>
</td>
        <td style="padding-left: 6px; padding-right: 6px; text-align: center; vertical-align: middle;">
<h1 class="post-title"> <span class="font-weight-bold">Arie</span> </h1>
</td>
        <td style="padding-left: 6px; text-align: center; vertical-align: middle;">
<h1 class="post-title">Nugraha</h1>
</td>
      </tr>
    </table>
    

<!--     <h1 class="post-title">
     Aditya <span class="font-weight-bold">Arie</span> Nugraha
    </h1> -->

     <p class="desc">Research Scientist @ <a href="https://www.riken.jp/en/research/labs/aip/index.html" target="_blank" rel="noopener noreferrer">RIKEN-AIP</a> (Japan) — Dr. in Informatics from University of Lorraine (France)</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        


<img class="img-fluid z-depth-1 rounded" src="/assets/resized/prof_pic-800x1000.jpg" srcset="    /assets/resized/prof_pic-480x600.jpg 480w,    /assets/resized/prof_pic-800x1000.jpg 800w,/ assets/img/prof_pic.jpg 960w" alt="prof_pic.jpg">

      
      
        <div class="address">
          <p><small>Kyoto University Artificial Intelligence Research Unit,</small></p> <p><small>Dr. Ichikawa Commemorative Laboratory, Room #202,</small></p> <p><small>Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501 JAPAN</small></p>

        </div>
      
    </div>
    

    <div class="clearfix">
      <p>I am a research scientist in the <a href="http://www.riken.jp/en/research/labs/aip/goalorient_tech/sound_scene_understand/" target="_blank" rel="noopener noreferrer">Sound Scene Understanding Team</a>, <a href="https://www.riken.jp/en/research/labs/aip/index.html" target="_blank" rel="noopener noreferrer">Center for Advanced Intelligence Project (AIP)</a>, <a href="http://www.riken.jp/en/" target="_blank" rel="noopener noreferrer">RIKEN</a> and a visiting researcher in the <a href="http://sap.ist.i.kyoto-u.ac.jp/EN/" target="_blank" rel="noopener noreferrer">Speech and Audio Processing Group</a>, <a href="https://www.kyoto-u.ac.jp/en/" target="_blank" rel="noopener noreferrer">Kyoto University</a>.</p>

<p>I received a Doctorate in Informatics from <a href="http://iaem.univ-lorraine.fr/" target="_blank" rel="noopener noreferrer">University of Lorraine</a>, France for a doctoral research on multichannel audio source separation based on deep neural networks conducted at <a href="https://team.inria.fr/multispeech/" target="_blank" rel="noopener noreferrer">Inria Nancy – Grand-Est</a>, France under the supervision of <a href="https://members.loria.fr/ALiutkus/" target="_blank" rel="noopener noreferrer">Dr. Antoine Liutkus</a> and <a href="https://members.loria.fr/EVincent/" target="_blank" rel="noopener noreferrer">Dr. Emmanuel Vincent</a>. The <a href="https://www.theses.fr/en/2017LORR0212" target="_blank" rel="noopener noreferrer">doctoral thesis</a> covers the applications of our separation methods to various tasks, including speech enhancement, singing voice separation, and musical instrument separation.</p>

<p>My current research interests include <i>audio source separation</i>, <i>audio-visual scene understanding</i>, and <i>machine learning</i>.</p>

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com){:target="\_blank"}. You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/){:target="\_blank"} and [Academicons](https://jpswalsh.github.io/academicons/){:target="\_blank"}, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

    </div>

    

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Oct 24, 2023</th>
          <td><a class="news-title" href="/news/waspaa2023_presentation/">Our paper “Time-Domain Audio Source Separation Based on Gaussian Processes with Deep Kernel Learning” was presented at IEEE WASPAA 2023.</a></td>
        </tr>
      
        <tr>
          <th scope="row">Aug 20, 2023</th>
          <td><a class="news-title" href="/news/interspeech23_tutorial/">Our team provided a tutorial entitled “Foundations, Extensions and Applications of Statistical Multichannel Speech Separation Models” at Interspeech 2023.</a></td>
        </tr>
      
        <tr>
          <th scope="row">Jun 8, 2023</th>
          <td><a class="news-title" href="/news/icassp2023_presentation/">Our paper “Exploiting Sparse Recovery Algorithms for Semi-Supervised Training of Deep Neural Networks for Direction-of-Arrival Estimation” was presented at IEEE ICASSP 2023.</a></td>
        </tr>
      
        <tr>
          <th scope="row">Oct 26, 2022</th>
          <td><a class="news-title" href="/news/iros2022_presentation/">Our paper “Direction-Aware Adaptive Online Neural Speech Enhancement with an Augmented Reality Headset in Real Noisy Conversational Environments” was presented at IEEE/RSJ IROS 2022.</a></td>
        </tr>
      
        <tr>
          <th scope="row">Sep 21, 2022</th>
          <td><a class="news-title" href="/news/interspeech2022_presentation/">Our paper “Direction-Aware Joint Adaptation of Neural Speech Enhancement and Recognition in Real Multiparty Conversational Environments” was presented at Interspeech 2022.</a></td>
        </tr>
      
        <tr>
          <th scope="row">Sep 7, 2022</th>
          <td>We presented two papers at <a href="https://iwaenc2022.org/" target="_blank" rel="noopener noreferrer">IWAENC 2022</a>: ① <a href="https://ieeexplore.ieee.org/document/9914729" target="_blank" rel="noopener noreferrer">“DNN-free Low-Latency Adaptive Speech Enhancement Based on Frame-Online Beamforming Powered by Block-Online FastMNMF”</a> and ② <a href="https://ieeexplore.ieee.org/document/9914786" target="_blank" rel="noopener noreferrer">“Joint Localization and Synchronization of Distributed Camera-Attached Microphone Arrays for Indoor Scene Analysis”</a>.</td>
        </tr>
      
        <tr>
          <th scope="row">May 7, 2022</th>
          <td>Our <a href="https://aip.riken.jp/labs/goalorient_tech/sound_scene_understand/" target="_blank" rel="noopener noreferrer">Sound Scene Understanding Team</a> presented two papers at <a href="https://2022.ieeeicassp.org/" target="_blank" rel="noopener noreferrer">IEEE ICASSP 2022</a>: ① <a href="https://2022.ieeeicassp.org/view_paper.php?PaperNum=9073" target="_blank" rel="noopener noreferrer">“Flow-Based Fast Multichannel Nonnegative Matrix Factorization for Blind Source Separation”</a> and ② <a href="https://2022.ieeeicassp.org/view_paper.php?PaperNum=9302" target="_blank" rel="noopener noreferrer">“Neural Full-Rank Spatial Covariance Analysis for Blind Source Separation”</a>.</td>
        </tr>
      
        <tr>
          <th scope="row">Apr 28, 2022</th>
          <td>Our article “Generalized Fast Multichannel Nonnegative Matrix Factorization Based on Gaussian Scale Mixtures for Blind Source Separation” has been accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing.
It is now available on <a href="https://ieeexplore.ieee.org/abstract/document/9769993" target="_blank" rel="noopener noreferrer">IEEE Xplore</a>.</td>
        </tr>
      
        <tr>
          <th scope="row">Apr 1, 2022</th>
          <td>I’m happy to share that I’m starting a new position as Research Scientist (研究員) at RIKEN!</td>
        </tr>
      
        <tr>
          <th scope="row">Jan 22, 2022</th>
          <td>Our paper “Flow-Based Fast Multichannel Nonnegative Matrix Factorization for Blind Source Separation” has been accepted to <a href="https://2022.ieeeicassp.org/" target="_blank" rel="noopener noreferrer">IEEE ICASSP 2022</a>.</td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WASPAA</abbr>
    
  
  
  </div>

  <div id="nugraha23gpdkl" class="col-sm-9">
    
      <div class="title">Time-Domain Audio Source Separation Based on Gaussian Processes with Deep Kernel Learning</div>
      <div class="author">
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha</em>,
                
              
            
          
        
          
          
          
          
          
          
          
            
              
                
                  
                    <!-- Carlo, Diego Di, -->
                    Diego Di Carlo,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://ybando.jp/" target="_blank">Bando, Yoshiaki</a>, -->
                    <a href="https://ybando.jp/" target="_blank" rel="noopener noreferrer">Yoshiaki Bando</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://matfontaine.github.io/" target="_blank">Fontaine, Mathieu</a>, -->
                    <a href="https://matfontaine.github.io/" target="_blank" rel="noopener noreferrer">Mathieu Fontaine</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank">Yoshii, Kazuyoshi</a> -->
                  and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank" rel="noopener noreferrer">Kazuyoshi Yoshii</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em>,
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1109/WASPAA58266.2023.10248168" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="https://ieeexplore.ieee.org/document/10248168" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      <a href="https://hal.science/hal-04172863" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Preprint</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper revisits single-channel audio source separation based on a probabilistic generative model of a mixture signal defined in the continuous time domain. We assume that each source signal follows a non-stationary Gaussian process (GP), i.e., any finite set of sampled points follows a zero-mean multivariate Gaussian distribution whose covariance matrix is governed by a kernel function over time-varying latent variables. The mixture signal composed of such source signals thus follows a GP whose covariance matrix is given by the sum of the source covariance matrices. To estimate the latent variables from the mixture signal, we use a deep neural network with an encoder-separator-decoder architecture (e.g., Conv-TasNet) that separates the latent variables in a pseudo-time-frequency space. The key feature of our method is to feed the latent variables into the kernel function for estimating the source covariance matrices, instead of using the decoder for directly estimating the time-domain source signals. This enables the decomposition of a mixture signal into the source signals with a classical yet powerful Wiener filter that considers the full covariance structure over all samples. The kernel function and the network are trained jointly in the maximum likelihood framework. Comparative experiments using two-speech mixtures under clean, noisy, and noisy-reverberant conditions from the WSJ0-2mix, WHAM!, and WHAMR! benchmark datasets demonstrated that the proposed method performed well and outperformed the baseline method under noisy and noisy-reverberant conditions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nugraha23gpdkl</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{WASPAA}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nugraha, Aditya Arie and Carlo, Diego Di and Bando, Yoshiaki and Fontaine, Mathieu and Yoshii, Kazuyoshi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Time-Domain Audio Source Separation Based on Gaussian Processes with Deep Kernel Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--5}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New Paltz, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/10248168}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/10248168}</span><span class="p">,</span>
  <span class="na">preprint</span> <span class="p">=</span> <span class="s">{https://hal.science/hal-04172863}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/WASPAA58266.2023.10248168}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IROS</abbr>
    
  
  
  </div>

  <div id="sekiguchi22directionaware" class="col-sm-9">
    
      <div class="title">Direction-Aware Adaptive Online Neural Speech Enhancement with an Augmented Reality Headset in Real Noisy Conversational Environments</div>
      <div class="author">
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank">Sekiguchi*, Kouhei</a>, -->
                    <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank" rel="noopener noreferrer">Kouhei Sekiguchi*</a>,
                  
                
              
            
          
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha*, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha*</em>,
                
              
            
          
        
          
          
          
          
          
          
          
            
              
                
                  
                    <!-- Du, Yicheng, -->
                    Yicheng Du,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://ybando.jp/" target="_blank">Bando, Yoshiaki</a>, -->
                    <a href="https://ybando.jp/" target="_blank" rel="noopener noreferrer">Yoshiaki Bando</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://matfontaine.github.io/" target="_blank">Fontaine, Mathieu</a>, -->
                    <a href="https://matfontaine.github.io/" target="_blank" rel="noopener noreferrer">Mathieu Fontaine</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank">Yoshii, Kazuyoshi</a> -->
                  and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank" rel="noopener noreferrer">Kazuyoshi Yoshii</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)</em>,
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1109/IROS47612.2022.9981659" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="https://ieeexplore.ieee.org/document/9981659" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      <a href="https://arxiv.org/abs/2207.07296" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Preprint</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes the practical response- and performance-aware development of online speech enhancement for an augmented reality (AR) headset that helps a user understand conversations made in real noisy echoic environments (e.g., cocktail party). One may use a state-of-the-art blind source separation method called fast multichannel nonnegative matrix factorization (FastMNMF) that works well in various environments thanks to its unsupervised nature. Its heavy computational cost, however, prevents its application to real-time processing. In contrast, a supervised beamforming method that uses a deep neural network (DNN) for estimating spatial information of speech and noise readily fits real-time processing, but suffers from drastic performance degradation in mismatched conditions. Given such complementary characteristics, we propose a dual-process robust online speech enhancement method based on DNN-based beamforming with FastMNMF-guided adaptation. FastMNMF (back end) is performed in a mini-batch style and the noisy and enhanced speech pairs are used together with the original parallel training data for updating the direction-aware DNN (front end) with backpropagation at a computationally-allowable interval. This method is used with a blind dereverberation method called weighted prediction error (WPE) for transcribing the noisy reverberant speech of a speaker, which can be detected from video or selected by a user’s hand gesture or eye gaze, in a streaming manner and spatially showing the transcriptions with an AR technique. Our experiment showed that the word error rate was improved by more than 10 points with the run-time adaptation using only twelve minutes of observation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sekiguchi22directionaware</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sekiguchi*, Kouhei and Nugraha*, Aditya Arie and Du, Yicheng and Bando, Yoshiaki and Fontaine, Mathieu and Yoshii, Kazuyoshi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Direction-Aware Adaptive Online Neural Speech Enhancement with an Augmented Reality Headset in Real Noisy Conversational Environments}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9266--9273}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Kyoto, Japan}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9981659}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9981659}</span><span class="p">,</span>
  <span class="na">preprint</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2207.07296}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS47612.2022.9981659}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IWAENC</abbr>
    
  
  
  </div>

  <div id="nugraha22dnnfree" class="col-sm-9">
    
      <div class="title">DNN-Free Low-Latency Adaptive Speech Enhancement Based on Frame-Online Beamforming Powered by Block-Online FastMNMF</div>
      <div class="author">
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha</em>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank">Sekiguchi, Kouhei</a>, -->
                    <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank" rel="noopener noreferrer">Kouhei Sekiguchi</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://matfontaine.github.io/" target="_blank">Fontaine, Mathieu</a>, -->
                    <a href="https://matfontaine.github.io/" target="_blank" rel="noopener noreferrer">Mathieu Fontaine</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://ybando.jp/" target="_blank">Bando, Yoshiaki</a>, -->
                    <a href="https://ybando.jp/" target="_blank" rel="noopener noreferrer">Yoshiaki Bando</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank">Yoshii, Kazuyoshi</a> -->
                  and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank" rel="noopener noreferrer">Kazuyoshi Yoshii</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of International Workshop on Acoustic Signal Enhancement (IWAENC)</em>,
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1109/IWAENC53105.2022.9914729" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="https://ieeexplore.ieee.org/document/9914729" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      <a href="https://arxiv.org/abs/2207.10934" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Preprint</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes a practical dual-process speech enhancement system that adapts environment-sensitive frame-online beamforming (front-end) with help from environment-free block-online source separation (back-end). To use minimum variance distortionless response (MVDR) beamforming, one may train a deep neural network (DNN) that estimates time-frequency masks used for computing the covariance matrices of sources (speech and noise). Backpropagation-based run-time adaptation of the DNN was proposed for dealing with the mismatched training-test conditions. Instead, one may try to directly estimate the source covariance matrices with a state-of-the-art blind source separation method called fast multichannel non-negative matrix factorization (FastMNMF). In practice, however, neither the DNN nor the FastMNMF can be updated in a frame-online manner due to its computationally-expensive iterative nature. Our DNN-free system leverages the posteriors of the latest source spectrograms given by block-online FastMNMF to derive the current source covariance matrices for frame-online beamforming. The evaluation shows that our frame-online system can quickly respond to scene changes caused by interfering speaker movements and outperformed an existing block-online system with DNN-based beamforming by 5.0 points in terms of the word error rate.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nugraha22dnnfree</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IWAENC}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nugraha, Aditya Arie and Sekiguchi, Kouhei and Fontaine, Mathieu and Bando, Yoshiaki and Yoshii, Kazuyoshi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DNN-Free Low-Latency Adaptive Speech Enhancement Based on Frame-Online Beamforming Powered by Block-Online FastMNMF}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of International Workshop on Acoustic Signal Enhancement (IWAENC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--5}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Bamberg, Germany}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9914729}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9914729}</span><span class="p">,</span>
  <span class="na">preprint</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2207.10934}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IWAENC53105.2022.9914729}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  
  </div>

  <div id="nugraha22nffastmnmf" class="col-sm-9">
    
      <div class="title">Flow-Based Fast Multichannel Nonnegative Matrix Factorization for Blind Source Separation</div>
      <div class="author">
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha</em>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank">Sekiguchi, Kouhei</a>, -->
                    <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank" rel="noopener noreferrer">Kouhei Sekiguchi</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://matfontaine.github.io/" target="_blank">Fontaine, Mathieu</a>, -->
                    <a href="https://matfontaine.github.io/" target="_blank" rel="noopener noreferrer">Mathieu Fontaine</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://ybando.jp/" target="_blank">Bando, Yoshiaki</a>, -->
                    <a href="https://ybando.jp/" target="_blank" rel="noopener noreferrer">Yoshiaki Bando</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank">Yoshii, Kazuyoshi</a> -->
                  and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank" rel="noopener noreferrer">Kazuyoshi Yoshii</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>,
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1109/ICASSP43922.2022.9747718" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="https://ieeexplore.ieee.org/document/9747718" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      <a href="https://hal.archives-ouvertes.fr/hal-03637425/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Preprint</a>
    
    
    
    
    
    
      
      <a href="https://sigport.org/documents/flow-based-fast-multichannel-nonnegative-matrix-factorization-blind-source-separation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes a blind source separation method for multichannel audio signals, called NF-FastMNMF, based on the integration of the normalizing flow (NF) into the multichannel nonnegative matrix factorization with jointly-diagonalizable spatial covariance matrices, a.k.a. FastMNMF. Whereas the NF of flow-based independent vector analysis, called NF-IVA, acts as the demixing matrices to transform an M-channel mixture into M independent sources, the NF of NF-FastMNMF acts as the diagonalization matrices to transform an M- channel mixture into a spatially-independent M-channel mixture represented as a weighted sum of N source images. This diagonalization enables the NF, which has been used only for determined separation because of its bijective nature, to be applicable to non-determined separation. NF-FastMNMF has time-varying diagonalization matrices that are potentially better at handling dynamical data variation than the time-invariant ones in FastMNMF. To have an NF with richer expression capability, the dimension-wise scalings using diagonal matrices originally used in NF-IVA are replaced with linear transformations using upper triangular matrices; in both cases, the diagonal and upper triangular matrices are estimated by neural networks. The evaluation shows that NF-FastMNMF performs well for both determined and non-determined separations of multiple speech utterances by stationary or non-stationary speakers from a noisy reverberant mixture.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nugraha22nffastmnmf</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICASSP}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nugraha, Aditya Arie and Sekiguchi, Kouhei and Fontaine, Mathieu and Bando, Yoshiaki and Yoshii, Kazuyoshi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flow-Based Fast Multichannel Nonnegative Matrix Factorization for Blind Source Separation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{501--505}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9747718}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9747718}</span><span class="p">,</span>
  <span class="na">preprint</span> <span class="p">=</span> <span class="s">{https://hal.archives-ouvertes.fr/hal-03637425/}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{https://sigport.org/documents/flow-based-fast-multichannel-nonnegative-matrix-factorization-blind-source-separation}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP43922.2022.9747718}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SPL</abbr>
    
  
  
  </div>

  <div id="bando21neuralfca" class="col-sm-9">
    
      <div class="title">Neural Full-Rank Spatial Covariance Analysis for Blind Source Separation</div>
      <div class="author">
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://ybando.jp/" target="_blank">Bando, Yoshiaki</a>, -->
                    <a href="https://ybando.jp/" target="_blank" rel="noopener noreferrer">Yoshiaki Bando</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank">Sekiguchi, Kouhei</a>, -->
                    <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank" rel="noopener noreferrer">Kouhei Sekiguchi</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://sites.google.com/view/yoshiki-masuyama/home" target="_blank">Masuyama, Yoshiki</a>, -->
                    <a href="https://sites.google.com/view/yoshiki-masuyama/home" target="_blank" rel="noopener noreferrer">Yoshiki Masuyama</a>,
                  
                
              
            
          
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha</em>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://matfontaine.github.io/" target="_blank">Fontaine, Mathieu</a>, -->
                    <a href="https://matfontaine.github.io/" target="_blank" rel="noopener noreferrer">Mathieu Fontaine</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank">Yoshii, Kazuyoshi</a> -->
                  and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank" rel="noopener noreferrer">Kazuyoshi Yoshii</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Signal Processing Letters</em>,
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1109/LSP.2021.3101699" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="https://ieeexplore.ieee.org/document/9506855" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
      
      <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9506855" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes aneural blind source separation (BSS) method based on amortized variational inference (AVI) of a non-linear generative model of mixture signals. A classical statistical approach to BSS is to fit a linear generative model that consists of spatial and source models representing the inter-channel covariances and power spectral densities of sources, respectively. Although the variational autoencoder (VAE) has successfully been used as a non-linear source model with latent features, it should be pretrained from a sufficient amount of isolated signals. Our method, in contrast, enables the VAE-based source model to be trained only from mixture signals. Specifically, we introduce a neural mixture-to-feature inference model that directly infers the latent features from the observed mixture and integrate it with a neural feature-to-mixture generative model consisting of a full-rank spatial model and a VAE-based source model. All the models are optimized jointly such that the likelihood for the training mixtures is maximized in the framework of AVI. Once the inference model is optimized, it can be used for estimating the latent features of sources included in unseen mixture signals. The experimental results show that the proposed method outperformed the state-of-the-art BSS methods based on linear generative models and was comparable to a method based on supervised learning of the VAE-based sourcemodel.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bando21neuralfca</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{SPL}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bando, Yoshiaki and Sekiguchi, Kouhei and Masuyama, Yoshiki and Nugraha, Aditya Arie and Fontaine, Mathieu and Yoshii, Kazuyoshi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Signal Processing Letters}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Full-Rank Spatial Covariance Analysis for Blind Source Separation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{28}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1670--1674}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9506855}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9506855}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9506855}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LSP.2021.3101699}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TASLP</abbr>
    
  
  
    <p class="award badge">15th IEEE Signal Processing Society (SPS) Japan Student Journal Paper Award</p>
  
  </div>

  <div id="sekiguchi20fastmnmf" class="col-sm-9">
    
      <div class="title">Fast Multichannel Nonnegative Matrix Factorization with Directivity-Aware Jointly-Diagonalizable Spatial Covariance Matrices for Blind Source Separation</div>
      <div class="author">
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank">Sekiguchi, Kouhei</a>, -->
                    <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank" rel="noopener noreferrer">Kouhei Sekiguchi</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://ybando.jp/" target="_blank">Bando, Yoshiaki</a>, -->
                    <a href="https://ybando.jp/" target="_blank" rel="noopener noreferrer">Yoshiaki Bando</a>,
                  
                
              
            
          
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha</em>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank">Yoshii, Kazuyoshi</a>, -->
                    <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank" rel="noopener noreferrer">Kazuyoshi Yoshii</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Kawahara, Tatsuya</a> -->
                  and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank" rel="noopener noreferrer">Tatsuya Kawahara</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>,
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1109/TASLP.2020.3019181" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="https://ieeexplore.ieee.org/document/9177266" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
      
      <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9177266" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/sekiguchi92/SoundSourceSeparation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes a computationally-efficient blind source separation (BSS) method based on the independence, low-rankness, and directivity of the sources. A typical approach to BSS is unsupervised learning of a probabilistic model that consists of a source model representing the time-frequency structure of source images and a spatial model representing their inter-channel covariance structure. Building upon the low-rank source model based on nonnegative matrix factorization (NMF), which has been considered to be effective for inter-frequency source alignment, multichannel NMF (MNMF) assumes source images to follow multivariate complex Gaussian distributions with unconstrained full-rank spatial covariance matrices (SCMs). An effective way of reducing the computational cost and initialization sensitivity of MNMF is to restrict the degree of freedom of SCMs. While a variant of MNMF called independent low-rank matrix analysis (ILRMA) severely restricts SCMs to rank-1 matrices under an idealized condition that only directional and less-echoic sources exist, we restrict SCMs to jointly-diagonalizable yet full-rank matrices in a frequency-wise manner, resulting in FastMNMF1. To help inter-frequency source alignment, we then propose FastMNMF2 that shares the directional feature of each source over all frequency bins. To explicitly consider the directivity or diffuseness of each source, we also propose rank-constrained FastMNMF that enables us to individually specify the ranks of SCMs. Our experiments showed the superiority of FastMNMF over MNMF and ILRMA in speech separation and the effectiveness of the rank constraint in speech enhancement.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sekiguchi20fastmnmf</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{TASLP}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fast Multichannel Nonnegative Matrix Factorization with Directivity-Aware Jointly-Diagonalizable Spatial Covariance Matrices for Blind Source Separation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sekiguchi, Kouhei and Bando, Yoshiaki and Nugraha, Aditya Arie and Yoshii, Kazuyoshi and Kawahara, Tatsuya}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE/ACM} Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{28}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2610--2625}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9177266}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9177266}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9177266}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/sekiguchi92/SoundSourceSeparation}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TASLP.2020.3019181}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{15th IEEE Signal Processing Society (SPS) Japan Student Journal Paper Award}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SPL</abbr>
    
  
  
  </div>

  <div id="nugraha20nfiva" class="col-sm-9">
    
      <div class="title">Flow-Based Independent Vector Analysis for Blind Source Separation</div>
      <div class="author">
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha</em>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank">Sekiguchi, Kouhei</a>, -->
                    <a href="http://sap.ist.i.kyoto-u.ac.jp/members/sekiguch/" target="_blank" rel="noopener noreferrer">Kouhei Sekiguchi</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://matfontaine.github.io/" target="_blank">Fontaine, Mathieu</a>, -->
                    <a href="https://matfontaine.github.io/" target="_blank" rel="noopener noreferrer">Mathieu Fontaine</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://ybando.jp/" target="_blank">Bando, Yoshiaki</a>, -->
                    <a href="https://ybando.jp/" target="_blank" rel="noopener noreferrer">Yoshiaki Bando</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank">Yoshii, Kazuyoshi</a> -->
                  and <a href="http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/" target="_blank" rel="noopener noreferrer">Kazuyoshi Yoshii</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Signal Processing Letters</em>,
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1109/LSP.2020.3039944" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="https://ieeexplore.ieee.org/document/9269436" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
      
      <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9269436" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes a time-varying extension of independent vector analysis (IVA) based on the normalizing flow (NF), called NF-IVA, for determined blind source separation of multichannel audio signals. As in IVA, NF-IVA estimates demixing matrices that transform mixture spectra to source spectra in the complex-valued spatial domain such that the likelihood of those matrices for the mixture spectra is maximized under some non-Gaussian source model. While IVA performs a time-invariant bijective linear transformation, NF-IVA performs a series of time-varying bijective linear transformations (flow blocks) adaptively predicted by neural networks. To regularize such transformations, we introduce a soft volume-preserving (VP) constraint. Given mixture spectra, the parameters of NF-IVA are optimized by gradient descent with backpropagation in an unsupervised manner. Experimental results show that NF-IVA successfully performs speech separation in reverberant environments with different numbers of speakers and microphones and that NF-IVA with the VP constraint outperforms NF-IVA without it, standard IVA with iterative projection, and improved IVA with gradient descent.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nugraha20nfiva</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{SPL}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flow-Based Independent Vector Analysis for Blind Source Separation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nugraha, Aditya Arie and Sekiguchi, Kouhei and Fontaine, Mathieu and Bando, Yoshiaki and Yoshii, Kazuyoshi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Signal Processing Letters}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{27}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2173--2177}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9269436}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9269436}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9269436}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LSP.2020.3039944}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CSL</abbr>
    
  
  
    <p class="award badge">ISCA Award for the Best Review Paper published in Computer Speech and Language (2016-2020)</p>
  
  </div>

  <div id="vincent17csl" class="col-sm-9">
    
      <div class="title">An analysis of environment, microphone and data simulation mismatches in robust speech recognition</div>
      <div class="author">
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://members.loria.fr/EVincent/" target="_blank">Vincent, Emmanuel</a>, -->
                    <a href="https://members.loria.fr/EVincent/" target="_blank" rel="noopener noreferrer">Emmanuel Vincent</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Watanabe, Shinji</a>, -->
                    <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank" rel="noopener noreferrer">Shinji Watanabe</a>,
                  
                
              
            
          
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha</em>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank">Barker, Jon</a>, -->
                    <a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer">Jon Barker</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="https://www.ricardmarxer.com/" target="_blank">Marxer, Ricard</a> -->
                  and <a href="https://www.ricardmarxer.com/" target="_blank" rel="noopener noreferrer">Ricard Marxer</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computer Speech &amp; Language</em>,
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1016/j.csl.2016.11.005" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="http://www.sciencedirect.com/science/article/pii/S0885230816301231" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      <a href="https://hal.inria.fr/hal-01399180" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Preprint</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Speech enhancement and automatic speech recognition (ASR) are most often evaluated in matched (or multi-condition) settings where the acoustic conditions of the training data match (or cover) those of the test data. Few studies have systematically assessed the impact of acoustic mismatches between training and test data, especially concerning recent speech enhancement and state-of-the-art ASR techniques. In this article, we study this issue in the context of the CHiME-3 dataset, which consists of sentences spoken by talkers situated in challenging noisy environments recorded using a 6-channel tablet based microphone array. We provide a critical analysis of the results published on this dataset for various signal enhancement, feature extraction, and ASR backend techniques and perform a number of new experiments in order to separately assess the impact of different noise environments, different numbers and positions of microphones, or simulated vs. real data on speech enhancement and ASR performance. We show that, with the exception of minimum variance distortionless response (MVDR) beamforming, most algorithms perform consistently on real and simulated data and can benefit from training on simulated data. We also find that training on different noise environments and different microphones barely affects the ASR performance, especially when several environments are present in the training data: only the number of microphones has a significant impact. Based on these results, we introduce the CHiME-4 Speech Separation and Recognition Challenge, which revisits the CHiME-3 dataset and makes it more challenging by reducing the number of microphones available for testing.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vincent17csl</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CSL}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An analysis of environment, microphone and data simulation mismatches in robust speech recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vincent, Emmanuel and Watanabe, Shinji and Nugraha, Aditya Arie and Barker, Jon and Marxer, Ricard}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Speech &amp; Language}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{535--557}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www.sciencedirect.com/science/article/pii/S0885230816301231}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{http://www.sciencedirect.com/science/article/pii/S0885230816301231}</span><span class="p">,</span>
  <span class="na">preprint</span> <span class="p">=</span> <span class="s">{https://hal.inria.fr/hal-01399180}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.csl.2016.11.005}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{ISCA Award for the Best Review Paper published in Computer Speech and Language (2016-2020)}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TASLP</abbr>
    
  
  
    <p class="award badge">6th IEEE Signal Processing Society (SPS) Japan Young Author Best Paper Award</p>
  
  </div>

  <div id="nugraha16massdnn" class="col-sm-9">
    
      <div class="title">Multichannel audio source separation with deep neural networks</div>
      <div class="author">
        
          
          
          
          
          
          
          
            
              
                
                  <!-- <em>Nugraha, Aditya Arie</em>, -->
                  <em>Aditya Arie Nugraha</em>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  
                    <!-- <a href="https://members.loria.fr/ALiutkus/" target="_blank">Liutkus, Antoine</a>, -->
                    <a href="https://members.loria.fr/ALiutkus/" target="_blank" rel="noopener noreferrer">Antoine Liutkus</a>,
                  
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <!-- and <a href="https://members.loria.fr/EVincent/" target="_blank">Vincent, Emmanuel</a> -->
                  and <a href="https://members.loria.fr/EVincent/" target="_blank" rel="noopener noreferrer">Emmanuel Vincent</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>,
      
      
        2016
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://doi.org/10.1109/TASLP.2016.2580946" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    
      <a href="http://ieeexplore.ieee.org/document/7492604" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      <a href="https://hal.inria.fr/hal-01163369" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Preprint</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This article addresses the problem of multichannel audio source separation. We propose a framework where deep neural networks (DNNs) are used to model the source spectra and combined with the classical multichannel Gaussian model to exploit the spatial information. The parameters are estimated in an iterative expectation-maximization (EM) fashion and used to derive a multichannel Wiener filter. We present an extensive experimental study to show the impact of different design choices on the performance of the proposed technique. We consider different cost functions for the training of DNNs, namely the probabilistically motivated Itakura-Saito divergence, and also Kullback-Leibler, Cauchy, mean squared error, and phase-sensitive cost functions. We also study the number of EM iterations and the use of multiple DNNs, where each DNN aims to improve the spectra estimated by the preceding EM iteration. Finally, we present its application to a speech enhancement problem. The experimental results show the benefit of the proposed multichannel approach over a single-channel DNNbased approach and the conventional multichannel nonnegative matrix factorization based iterative EM algorithm.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nugraha16massdnn</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{TASLP}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multichannel audio source separation with deep neural networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nugraha, Aditya Arie and Liutkus, Antoine and Vincent, Emmanuel}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE/ACM} Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1652--1664}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://ieeexplore.ieee.org/document/7492604}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{http://ieeexplore.ieee.org/document/7492604}</span><span class="p">,</span>
  <span class="na">preprint</span> <span class="p">=</span> <span class="s">{https://hal.inria.fr/hal-01163369}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TASLP.2016.2580946}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{6th IEEE Signal Processing Society (SPS) Japan Young Author Best Paper Award}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%61%61.%6E%75%67%72%61%68%61@%69%65%65%65.%6F%72%67" title="email"><i class="fas fa-envelope"></i></a>
<a href="https://orcid.org/0000-0001-5424-747X" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a>
<a href="https://scholar.google.com/citations?user=j40T6joAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/aanugraha" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/aanugraha" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/AArieNugraha" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>







<a href="https://gitlab.com/aanugraha" title="GitLab" target="_blank" rel="noopener noreferrer"><i class="fab fa-gitlab"></i></a>
<a href="https://dblp.org/pid/145/5298.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a>


<a href="https://aanugraha.github.io/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a>

      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Aditya Arie Nugraha.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: September 12, 2024.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N73YZC8R04"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-N73YZC8R04');
</script>






</html>
